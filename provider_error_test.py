import requests
import json
import time
import subprocess
from pathlib import Path

def test_model_health():
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–¥–æ—Ä–æ–≤—å–µ –º–æ–¥–µ–ª–µ–π"""
    print("üîç –¢–µ—Å—Ç –∑–¥–æ—Ä–æ–≤—å—è –º–æ–¥–µ–ª–µ–π Ollama")
    print("=" * 50)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(models)}")
            
            for model in models:
                name = model['name']
                size = model['size']
                size_gb = size / (1024**3)
                print(f"  üì¶ {name}: {size_gb:.1f}GB")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {response.status_code}")
            return False
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")
        return False
    
    return True

def test_individual_models():
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏"""
    test_prompt = "–û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º: —Ç–µ—Å—Ç"
    
    models_to_test = [
        "llama3.2:latest",  # –°–∞–º–∞—è –º–∞–ª–µ–Ω—å–∫–∞—è (2GB)
        "mistral:7b-instruct",  # –°—Ä–µ–¥–Ω—è—è (4.4GB)
        "deepseek-r1:7b",  # –°—Ä–µ–¥–Ω—è—è (4.7GB)
        "deepseek-r1:latest"  # –°–∞–º–∞—è –±–æ–ª—å—à–∞—è (5.2GB)
    ]
    
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
    print("=" * 50)
    
    results = {}
    
    for model in models_to_test:
        print(f"\nü§ñ –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å: {model}")
        print("-" * 30)
        
        try:
            # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏
            subprocess.run(['ollama', 'stop'], capture_output=True)
            time.sleep(1)
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç
            data = {
                "model": model,
                "prompt": test_prompt,
                "stream": False,
                "options": {
                    "num_ctx": 512,  # –ú–∞–ª–µ–Ω—å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
                    "num_batch": 1,
                    "temperature": 0
                }
            }
            
            start_time = time.time()
            response = requests.post(
                "http://localhost:11434/api/generate", 
                json=data, 
                timeout=15
            )
            
            elapsed = time.time() - start_time
            
            if response.status_code == 200:
                answer = response.json()['response'].strip()
                results[model] = {
                    "success": True,
                    "time": elapsed,
                    "answer": answer
                }
                print(f"‚úÖ –£—Å–ø–µ—Ö ({elapsed:.1f}s): {answer}")
            else:
                error_text = response.text[:200]
                results[model] = {
                    "success": False,
                    "error": f"HTTP {response.status_code}",
                    "details": error_text
                }
                print(f"‚ùå –û—à–∏–±–∫–∞ HTTP {response.status_code}")
                print(f"   {error_text}")
                
        except requests.exceptions.Timeout:
            results[model] = {
                "success": False,
                "error": "Timeout"
            }
            print("‚ùå –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞")
        except Exception as e:
            results[model] = {
                "success": False,
                "error": str(e)
            }
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        
        time.sleep(2)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Ç–µ—Å—Ç–∞–º–∏
    
    return results

def analyze_results(results):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–æ–≤"""
    print("\nüìä –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    print("=" * 50)
    
    successful = [m for m, r in results.items() if r.get('success')]
    failed = [m for m, r in results.items() if not r.get('success')]
    
    print(f"‚úÖ –†–∞–±–æ—Ç–∞—é—â–∏–µ –º–æ–¥–µ–ª–∏: {len(successful)}")
    for model in successful:
        r = results[model]
        print(f"  üì¶ {model}: {r['time']:.1f}s")
    
    if failed:
        print(f"\n‚ùå –ù–µ —Ä–∞–±–æ—Ç–∞—é—â–∏–µ –º–æ–¥–µ–ª–∏: {len(failed)}")
        for model in failed:
            r = results[model]
            print(f"  üì¶ {model}: {r['error']}")
            if 'details' in r:
                print(f"     –î–µ—Ç–∞–ª–∏: {r['details'][:100]}...")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print("\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
    
    if len(successful) == 0:
        print("‚ö†Ô∏è  –ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ:")
        print("   1. –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ RAM (–Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 8GB —Å–≤–æ–±–æ–¥–Ω–æ–π)")
        print("   2. –ù–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç –ª–∏ –∞–Ω—Ç–∏–≤–∏—Ä—É—Å Ollama")
        print("   3. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ Ollama: ollama serve")
    elif len(successful) < len(results):
        print("‚ö†Ô∏è  –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–∞–±–æ—á–∏–µ.")
        print("   –û–±—ã—á–Ω–æ –º–µ–Ω—å—à–∏–µ –º–æ–¥–µ–ª–∏ (llama3.2) —Ä–∞–±–æ—Ç–∞—é—Ç —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ")
    else:
        print("‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –æ—Ç–ª–∏—á–Ω–æ!")
    
    return successful[0] if successful else None

def test_problematic_model(model_name):
    """–î–µ—Ç–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    print(f"\nüî¨ –î–µ—Ç–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_name}")
    print("=" * 50)
    
    tests = [
        {"prompt": "—Ç–µ—Å—Ç", "tokens": 10, "name": "–ö–æ—Ä–æ—Ç–∫–∏–π –∑–∞–ø—Ä–æ—Å"},
        {"prompt": "–ß—Ç–æ —Ç–∞–∫–æ–µ –∑–∞–∫–æ–Ω?", "tokens": 50, "name": "–°—Ä–µ–¥–Ω–∏–π –∑–∞–ø—Ä–æ—Å"},
        {"prompt": "–†–∞—Å—Å–∫–∞–∂–∏—Ç–µ –ø–æ–¥—Ä–æ–±–Ω–æ –æ –≥—Ä–∞–∂–¥–∞–Ω—Å–∫–æ–º –ø—Ä–∞–≤–µ", "tokens": 200, "name": "–î–ª–∏–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å"}
    ]
    
    for test in tests:
        print(f"\nüìù {test['name']}: {test['prompt']}")
        print("-" * 30)
        
        try:
            data = {
                "model": model_name,
                "prompt": test['prompt'],
                "stream": False,
                "options": {
                    "max_tokens": test['tokens'],
                    "temperature": 0
                }
            }
            
            start_time = time.time()
            response = requests.post(
                "http://localhost:11434/api/generate", 
                json=data, 
                timeout=30
            )
            
            elapsed = time.time() - start_time
            
            if response.status_code == 200:
                answer = response.json()['response'].strip()
                print(f"‚úÖ –£—Å–ø–µ—Ö ({elapsed:.1f}s): {answer[:100]}...")
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞: {response.status_code}")
                print(f"   {response.text[:200]}...")
                
        except Exception as e:
            print(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}")

def main():
    print("üîß –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ Provider Error –≤ Ollama")
    print("=" * 50)
    
    # –®–∞–≥ 1: –ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
    if not test_model_health():
        return
    
    # –®–∞–≥ 2: –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏
    results = test_individual_models()
    
    # –®–∞–≥ 3: –ê–Ω–∞–ª–∏–∑
    working_model = analyze_results(results)
    
    # –®–∞–≥ 4: –î–µ—Ç–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    for model, result in results.items():
        if not result.get('success'):
            test_problematic_model(model)
            break
    
    print("\n" + "=" * 50)
    print("üèÅ –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

if __name__ == "__main__":
    main()