#!/usr/bin/env python3
"""
–ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ç–µ—Å—Ç Saiga Mistral 7B —á–µ—Ä–µ–∑ Hugging Face Transformers
"""

import os
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from huggingface_hub import snapshot_download
import sys
from tqdm import tqdm

# === –ü—É—Ç–∏ ===
PROJECT_ROOT = Path(__file__).parent.parent.resolve()
MODEL_DIR = PROJECT_ROOT / "models" / "saiga_mistral_7b_merged"
MODEL_ID = "IlyaGusev/saiga_mistral_7b_merged"

def check_and_download_model():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Å–∫–∞—á–∏–≤–∞–µ—Ç —á–µ—Ä–µ–∑ huggingface_hub"""
    if MODEL_DIR.exists() and any(MODEL_DIR.iterdir()):
        size_gb = sum(f.stat().st_size for f in MODEL_DIR.rglob('*') if f.is_file()) / (1024**3)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞: {MODEL_DIR}")
        print(f"   –†–∞–∑–º–µ—Ä: {size_gb:.2f} –ì–ë")
        return True
    
    choice = input(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {MODEL_DIR}\n–°–∫–∞—á–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏? (y/n): ").strip().lower()
    if choice != 'y':
        print(f"\nüîó –°–∫–∞—á–∞–π –≤—Ä—É—á–Ω—É—é:")
        print(f"huggingface-cli download {MODEL_ID} --local-dir {MODEL_DIR}")
        sys.exit(1)
    
    print(f"üì• –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å {MODEL_ID} (~14 –ì–ë)...")
    print("–≠—Ç–æ –∑–∞–π–º—ë—Ç 10‚Äì60 –º–∏–Ω—É—Ç. –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –æ—Ç huggingface_hub:")

    try:
        snapshot_download(
            repo_id=MODEL_ID,
            local_dir=MODEL_DIR,
            local_dir_use_symlinks=False,
            resume_download=True,
            tqdm_class=tqdm
        )


        print("‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞!")
        return True
    except Exception as e:
        print(f"\n–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏: {e}")
        print("–ü–æ–ø—Ä–æ–±—É–π –≤—É—á–Ω—É—é:")
        print(f"huggingface-cli download {MODEL_ID} --local-dir {MODEL_DIR}")
        sys.exit(1)

def load_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä"""
    print("üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...")
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_DIR,
        torch_dtype=torch.float16,  # FP16 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        device_map="auto",          # –ê–≤—Ç–æ-—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ GPU/CPU
        trust_remote_code=True
    )
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
    return model, tokenizer

def generate_response(model, tokenizer, prompt, max_new_tokens=300):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç"""
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response[len(prompt):].strip()  # –£–±–∏—Ä–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–º–ø—Ç

# === –û–°–ù–û–í–ù–û–ô –ö–û–î ===
if __name__ == "__main__":
    check_and_download_model()
    
    model, tokenizer = load_model()
    
    # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è Saiga
    prompt = """<|im_start|>system
–¢—ã Saiga ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.<|im_end|>
<|im_start|>user
–ß—Ç–æ —Ç–∞–∫–æ–µ —Ç—Ä—É–¥–æ–≤–æ–π –¥–æ–≥–æ–≤–æ—Ä –ø–æ –¢–ö –†–§?<|im_end|>
<|im_start|>assistant
"""
    
    print("\nü§ñ –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å...")
    response = generate_response(model, tokenizer, prompt)
    
    print("\nüìù –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:")
    print(response)
