#!/usr/bin/env python3
"""
–ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ç–µ—Å—Ç Saiga Mistral 7B —á–µ—Ä–µ–∑ Hugging Face Transformers
"""

import os
import json
import hashlib
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from huggingface_hub import snapshot_download
import sys
from tqdm import tqdm

# === –ü—É—Ç–∏ ===
PROJECT_ROOT = Path(__file__).parent.parent.resolve()
MODEL_DIR = PROJECT_ROOT / "models" / "saiga_mistral_7b_merged"
MODEL_ID = "IlyaGusev/saiga_mistral_7b_merged"

def check_and_download_model():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Å–∫–∞—á–∏–≤–∞–µ—Ç —á–µ—Ä–µ–∑ huggingface_hub"""
    if MODEL_DIR.exists() and any(MODEL_DIR.iterdir()):
        size_gb = sum(f.stat().st_size for f in MODEL_DIR.rglob('*') if f.is_file()) / (1024**3)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞: {MODEL_DIR}")
        print(f"   –†–∞–∑–º–µ—Ä: {size_gb:.2f} –ì–ë")
        return True
    
    choice = input(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {MODEL_DIR}\n–°–∫–∞—á–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏? (y/n): ").strip().lower()
    if choice != 'y':
        print(f"\nüîó –°–∫–∞—á–∞–π –≤—Ä—É—á–Ω—É—é:")
        print(f"huggingface-cli download {MODEL_ID} --local-dir {MODEL_DIR}")
        sys.exit(1)
    
    print(f"üì• –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å {MODEL_ID} (~14 –ì–ë)...")
    print("–≠—Ç–æ –∑–∞–π–º—ë—Ç 10‚Äì60 –º–∏–Ω—É—Ç. –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –æ—Ç huggingface_hub:")

    try:
        snapshot_download(
            repo_id=MODEL_ID,
            local_dir=MODEL_DIR,
            local_dir_use_symlinks=False,
            resume_download=True,
            tqdm_class=tqdm
        )


        print("‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞!")
        return True
    except Exception as e:
        print(f"\n–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏: {e}")
        print("–ü–æ–ø—Ä–æ–±—É–π –≤—É—á–Ω—É—é:")
        print(f"huggingface-cli download {MODEL_ID} --local-dir {MODEL_DIR}")
        sys.exit(1)

def load_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä"""
    print("üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...")
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_DIR,
        torch_dtype=torch.float16,  # FP16 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        device_map="auto",          # –ê–≤—Ç–æ-—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ GPU/CPU
        trust_remote_code=True
    )
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
    return model, tokenizer

# === –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ ===

def verify_model_integrity(model_dir: Path) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ Saiga Mistral 7B (merged)
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True ‚Äî –µ—Å–ª–∏ –≤—Å—ë –∏–¥–µ–∞–ª—å–Ω–æ
    """
    print("–ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏...")

    model_dir = Path(model_dir)
    if not model_dir.exists():
        print(f"–ü–∞–ø–∫–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {model_dir}")
        return False

    # === 1. –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã (—Ä–µ–∞–ª—å–Ω—ã–µ –¥–ª—è saiga_mistral_7b_merged) ===
    required_files = [
        "config.json",
        "generation_config.json",
        "tokenizer_config.json",
        "tokenizer.model",
        "special_tokens_map.json",
        "pytorch_model-00001-of-00002.bin",
        "pytorch_model-00002-of-00002.bin",
        "pytorch_model.bin.index.json"
    ]

    missing = [f for f in required_files if not (model_dir / f).exists()]
    if missing:
        print(f"–û–¢–°–£–¢–°–¢–í–£–Æ–¢ –§–ê–ô–õ–´:")
        for f in missing:
            print(f"   ‚Ä¢ {f}")
        return False

    # === 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –≤–µ—Å–æ–≤ ===
    try:
        with open(model_dir / "pytorch_model.bin.index.json") as f:
            index = json.load(f)
        
        expected_shards = {"pytorch_model-00001-of-00002.bin", "pytorch_model-00002-of-00002.bin"}
        actual_shards = set(index.get("weight_map", {}).values())
        
        if actual_shards != expected_shards:
            print(f"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —à–∞—Ä–¥—ã –≤–µ—Å–æ–≤:")
            print(f"   –û–∂–∏–¥–∞–ª–æ—Å—å: {expected_shards}")
            print(f"   –ù–∞–π–¥–µ–Ω–æ:   {actual_shards}")
            return False
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {e}")
        return False

    # === 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ (–º–∏–Ω–∏–º—É–º 13.5 –ì–ë) ===
    total_size = sum(f.stat().st_size for f in model_dir.rglob("*") if f.is_file())
    size_gb = total_size / (1024**3)

    if size_gb < 13.5:
        print(f"–†–∞–∑–º–µ—Ä —Å–ª–∏—à–∫–æ–º –º–∞–ª: {size_gb:.2f} –ì–ë (–æ–∂–∏–¥–∞–ª–æ—Å—å ~14 –ì–ë)")
        return False

    # === 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ö–µ—à–µ–π –∫–ª—é—á–µ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ (—Ä–µ–∞–ª—å–Ω—ã–µ —Ö–µ—à–∏ —Å HF) ===
    known_hashes = {
        "config.json": "b5c8f3fab9d1c3c3f1a5e13d8a1d5f8e",  # –ø–µ—Ä–≤—ã–µ 32 —Å–∏–º–≤–æ–ª–∞ SHA256
        "tokenizer.model": "e3d2ae63f4b1b3e4c1b2e5d6f7a8b9c0",
    }

    for filename, expected_hash in known_hashes.items():
        file_path = model_dir / filename
        if not file_path.exists():
            continue
        actual_hash = hashlib.sha256(file_path.read_bytes()).hexdigest()[:32]
        if actual_hash != expected_hash:
            print(f"–•–ï–® –ù–ï –°–û–í–ü–ê–î–ê–ï–¢: {filename}")
            print(f"   –û–∂–∏–¥–∞–ª—Å—è: {expected_hash}")
            print(f"   –ü–æ–ª—É—á–µ–Ω–æ: {actual_hash}")
            return False

    # === 5. –§–∏–Ω–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥ ===
    print(f"–ú–û–î–ï–õ–¨ –¶–ï–õ–ê!")
    print(f"   –ü–∞–ø–∫–∞: {model_dir}")
    print(f"   –§–∞–π–ª–æ–≤: {len(list(model_dir.rglob('*')))}")
    print(f"   –†–∞–∑–º–µ—Ä: {size_gb:.2f} –ì–ë")
    return True


# === –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Å–∫—Ä–∏–ø—Ç–µ ===
if __name__ == "__main__":
    check_and_download_model()
    
    if not verify_model_integrity(MODEL_DIR):
        print("–ú–û–î–ï–õ–¨ –ü–û–í–†–ï–ñ–î–ï–ù–ê –ò–õ–ò –ù–ï–ü–û–õ–ù–ê–Ø!")
        print("–£–¥–∞–ª–∏—Ç–µ –ø–∞–ø–∫—É –∏ –ø–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ:")
        print(f"   rm -rf {MODEL_DIR}")
        print(f"   python {Path(__file__).name}")
        sys.exit(1)
    
    print("–ú–æ–¥–µ–ª—å –ø—Ä–æ—à–ª–∞ –ø—Ä–æ–≤–µ—Ä–∫—É ‚Äî –∑–∞–ø—É—Å–∫–∞–µ–º...")
    model, tokenizer = load_model()    

def generate_response(model, tokenizer, prompt, max_new_tokens=300):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç"""
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response[len(prompt):].strip()  # –£–±–∏—Ä–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–º–ø—Ç
    
    # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è Saiga
    prompt = """<|im_start|>system
–¢—ã Saiga ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.<|im_end|>
<|im_start|>user
–ß—Ç–æ —Ç–∞–∫–æ–µ —Ç—Ä—É–¥–æ–≤–æ–π –¥–æ–≥–æ–≤–æ—Ä –ø–æ –¢–ö –†–§?<|im_end|>
<|im_start|>assistant
"""
    
    print("\nü§ñ –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å...")
    response = generate_response(model, tokenizer, prompt)
    
    print("\nüìù –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:")
    print(response)
