#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
–û—á–∏—Å—Ç–∫–∞ ‚Üí –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ 80/20 ‚Üí –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è ‚Üí –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
"""

import json
import re
import random
from pathlib import Path
from typing import List, Dict, Tuple
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    datefmt="%H:%M:%S"
)
logger = logging.getLogger(__name__)

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ NLTK
try:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
    russian_stopwords = set(stopwords.words('russian'))
except:
    logger.warning("NLTK resources not available, continuing without advanced preprocessing")
    russian_stopwords = set()

# –ü—É—Ç–∏
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
RAW_DATA_DIR = PROJECT_ROOT / "data" / "raw"
PROCESSED_DATA_DIR = PROJECT_ROOT / "data" / "processed"
PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)

def clean_accordion(text: str) -> str:
    """–£–¥–∞–ª–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∞–∫–∫–æ—Ä–¥–µ–æ–Ω–∞ –∏ –¥—Ä—É–≥–∏—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
    # –£–¥–∞–ª—è–µ–º —Ç–∏–ø–∏—á–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∞–∫–∫–æ—Ä–¥–µ–æ–Ω–∞
    text = re.sub(r'\s*\[.*?\]\s*', ' ', text)  # –£–¥–∞–ª—è–µ–º —Å–∫–æ–±–∫–∏ —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º
    text = re.sub(r'–ó–∞–∫—Ä—ã—Ç—å\s+[^\n]*', '', text)  # –£–¥–∞–ª—è–µ–º "–ó–∞–∫—Ä—ã—Ç—å" –∏ –ø–æ—Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç
    text = re.sub(r'–ü–æ–¥—Ä–æ–±–Ω–µ–µ\s+', '', text)  # –£–¥–∞–ª—è–µ–º "–ü–æ–¥—Ä–æ–±–Ω–µ–µ"
    text = re.sub(r'–ü–æ–∫–∞–∑–∞—Ç—å\s+', '', text)  # –£–¥–∞–ª—è–µ–º "–ü–æ–∫–∞–∑–∞—Ç—å"
    text = re.sub(r'–°–∫—Ä—ã—Ç—å\s+', '', text)  # –£–¥–∞–ª—è–µ–º "–°–∫—Ä—ã—Ç—å"
    text = re.sub(r'<[^>]+>', '', text)  # –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏
    text = re.sub(r'‚Üí|‚Üê|‚Üë|‚Üì|‚ñ∂|‚ñº|‚ñ∫|‚óÑ', '', text)  # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–µ–ª–∫–∏
    text = re.sub(r'üîó|üìå|‚úÖ|‚ùå|‚≠ê|‚ú®|‚ö°', '', text)  # –£–¥–∞–ª—è–µ–º —ç–º–æ–¥–∑–∏-–∏–∫–æ–Ω–∫–∏
    return text

def remove_emoji(text: str) -> str:
    """–£–¥–∞–ª–µ–Ω–∏–µ —ç–º–æ–¥–∑–∏ –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    # –£–¥–∞–ª—è–µ–º —ç–º–æ–¥–∑–∏
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # emoticons
        "\U0001F300-\U0001F5FF"  # symbols & pictographs
        "\U0001F680-\U0001F6FF"  # transport & map symbols
        "\U0001F1E0-\U0001F1FF"  # flags (iOS)
        "\U00002500-\U00002BEF"  # chinese char
        "\U00002702-\U000027B0"
        "\U00002702-\U000027B0"
        "\U000024C2-\U0001F251"
        "\U0001f926-\U0001f937"
        "\U00010000-\U0010ffff"
        "\u2640-\u2642"
        "\u2600-\u2B55"
        "\u200d"
        "\u23cf"
        "\u23e9"
        "\u231a"
        "\ufe0f"
        "\u3030"
        "]+", 
        flags=re.UNICODE
    )
    text = emoji_pattern.sub('', text)
    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def clean_text_advanced(text: str) -> str:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
    # –£–¥–∞–ª—è–µ–º —Å—Å—ã–ª–∫–∏
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
    # –£–¥–∞–ª—è–µ–º email
    text = re.sub(r'\S+@\S+', '', text)
    # –£–¥–∞–ª—è–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã–µ –Ω–æ–º–µ—Ä–∞
    text = re.sub(r'\+?7[0-9\-\(\)\s]{10,}', '', text)
    # –£–¥–∞–ª—è–µ–º –¥–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY
    text = re.sub(r'\d{1,2}\.\d{1,2}\.\d{4}', '', text)
    # –£–¥–∞–ª—è–µ–º —á–∏—Å–ª–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –º–æ–∂–Ω–æ –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –µ—Å–ª–∏ —á–∏—Å–ª–∞ –≤–∞–∂–Ω—ã)
    # text = re.sub(r'\d+', '', text)
    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def load_raw_data() -> List[Dict]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
    all_data = []
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ JSON —Ñ–∞–π–ª—ã –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ raw
    for json_file in RAW_DATA_DIR.glob("*.json"):
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑: {json_file.name}")
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–¥–µ–∫—Å–µ –∫ –∫–∞–∂–¥–æ–º—É —ç–ª–µ–º–µ–Ω—Ç—É
            code = json_file.stem.split('_')[0]  # –∏–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–¥ –∫–æ–¥–µ–∫—Å–∞ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'gk' –∏–∑ 'gk_100.json')
            for item in data:
                item['code'] = code  # –¥–æ–±–∞–≤–ª—è–µ–º –∫–æ–¥ –∫–æ–¥–µ–∫—Å–∞ –∫–∞–∫ –º–µ—Ç–∫—É
            
            all_data.extend(data)
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} —Å—Ç–∞—Ç–µ–π –∏–∑ {json_file.name}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {json_file.name}: {e}")
    
    logger.info(f"–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_data)} —Å—Ç–∞—Ç–µ–π –∏–∑ –≤—Å–µ—Ö –∫–æ–¥–µ–∫—Å–æ–≤")
    return all_data

def clean_data(raw_data: List[Dict]) -> List[Dict]:
    """–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö - —É–¥–∞–ª–µ–Ω–∏–µ –º—É—Å–æ—Ä–∞"""
    cleaned_data = []
    
    for item in raw_data:
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—á–∏—Å—Ç–∫–∏
        cleaned_content = clean_accordion(item['content'])
        cleaned_content = remove_emoji(cleaned_content)
        cleaned_content = clean_text_advanced(cleaned_content)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤ —ç–ª–µ–º–µ–Ω—Ç–µ
        cleaned_item = item.copy()
        cleaned_item['content'] = cleaned_content
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –ø—É—Å—Ç–æ–π –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏
        if cleaned_content.strip():
            cleaned_data.append(cleaned_item)
    
    logger.info(f"–ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –æ—Å—Ç–∞–ª–æ—Å—å {len(cleaned_data)} —Å—Ç–∞—Ç–µ–π –∏–∑ {len(raw_data)}")
    return cleaned_data

def split_by_codes(data: List[Dict], test_size: float = 0.2) -> Tuple[List[Dict], List[Dict]]:
    """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–æ–¥–µ–∫—Å–∞–º —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–π"""
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –∫–æ–¥–µ–∫—Å–∞–º
    code_groups = {}
    for item in data:
        code = item.get('code', 'unknown')
        if code not in code_groups:
            code_groups[code] = []
        code_groups[code].append(item)
    
    logger.info(f"–ù–∞–π–¥–µ–Ω—ã –∫–æ–¥–µ–∫—Å—ã: {list(code_groups.keys())}")
    
    train_data = []
    test_data = []
    
    for code, group in code_groups.items():
        logger.info(f"–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ {len(group)} —Å—Ç–∞—Ç–µ–π –¥–ª—è –∫–æ–¥–µ–∫—Å–∞ {code}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–ª—è —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ (—Ö–æ—Ç—è –≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –±—É–¥–µ—Ç –ø–æ –∫–æ–¥–µ–∫—Å–∞–º)
        contents = [item['content'] for item in group]
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        train_contents, test_contents = train_test_split(
            list(zip(group, contents)), 
            test_size=test_size, 
            random_state=42
        )
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ–ª–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –¥–∞–Ω–Ω—ã—Ö
        train_group = [item for item, _ in train_contents]
        test_group = [item for item, _ in test_contents]
        
        train_data.extend(train_group)
        test_data.extend(test_group)
        
        logger.info(f"  - –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(train_group)}")
        logger.info(f"  - –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(test_group)}")
    
    logger.info(f"–ò—Ç–æ–≥–æ: –æ–±—É—á–µ–Ω–∏–µ {len(train_data)}, —Ç–µ—Å—Ç {len(test_data)}")
    return train_data, test_data

def augment_data(data: List[Dict]) -> List[Dict]:
    """–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (—Å–∏–Ω–æ–Ω–∏–º—ã, –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∫–∏)"""
    augmented_data = []
    
    for item in data:
        augmented_data.append(item)  # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç
        
        # –ü—Ä–æ—Å—Ç–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è - –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        original_title = item['title']
        
        # –°–æ–∑–¥–∞–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–≥–æ–ª–æ–≤–∫–∞
        variations = [
            # –£–±–∏—Ä–∞–µ–º "–¥–µ–π—Å—Ç–≤—É—é—â–∞—è —Ä–µ–¥–∞–∫—Ü–∏—è" –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            re.sub(r'\s+\(–¥–µ–π—Å—Ç–≤—É—é—â–∞—è —Ä–µ–¥–∞–∫—Ü–∏—è\)', '', original_title),
            # –ó–∞–º–µ–Ω—è–µ–º "—Å—Ç–∞—Ç—å—è" –Ω–∞ "–ø–æ–ª–æ–∂–µ–Ω–∏–µ"
            re.sub(r'^–°—Ç–∞—Ç—å—è (\d+)', r'–ü–æ–ª–æ–∂–µ–Ω–∏–µ \1', original_title),
            # –£–±–∏—Ä–∞–µ–º –Ω—É–º–µ—Ä–∞—Ü–∏—é –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            re.sub(r'^–°—Ç–∞—Ç—å—è \d+\s*', '', original_title),
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –æ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª–∞
        for variation in variations:
            if variation != original_title and variation.strip():
                augmented_item = item.copy()
                augmented_item['title'] = variation
                augmented_item['augmentation_type'] = 'title_variation'
                augmented_data.append(augmented_item)
    
    logger.info(f"–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è: {len(data)} -> {len(augmented_data)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
    return augmented_data

def tokenize_data(data: List[Dict], model_name: str = "IlyaGusev/saiga_mistral_7b") -> List[Dict]:
    """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
    except:
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        from transformers import GPT2TokenizerFast
        tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")
    
    tokenized_data = []
    
    for item in data:
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (–æ–±—ä–µ–¥–∏–Ω—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ)
        text = f"{item['title']}\n\n{item['content']}"
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
        tokens = tokenizer(
            text,
            truncation=True,
            padding=False,
            max_length=2048,  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            return_tensors=None
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç
        tokenized_item = item.copy()
        tokenized_item['input_ids'] = tokens['input_ids']
        tokenized_item['attention_mask'] = tokens.get('attention_mask', [1] * len(tokens['input_ids']))
        tokenized_item['token_count'] = len(tokens['input_ids'])
        
        tokenized_data.append(tokenized_item)
    
    logger.info(f"–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞, —Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {sum(item['token_count'] for item in tokenized_data) / len(tokenized_data):.2f}")
    return tokenized_data

def save_processed_data(train_data: List[Dict], test_data: List[Dict]):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É
    train_file = PROCESSED_DATA_DIR / "train_data.json"
    with open(train_file, 'w', encoding='utf-8') as f:
        json.dump(train_data, f, ensure_ascii=False, indent=2)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É
    test_file = PROCESSED_DATA_DIR / "test_data.json"
    with open(test_file, 'w', encoding='utf-8') as f:
        json.dump(test_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
    logger.info(f"  - –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {train_file}")
    logger.info(f"  - –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {test_file}")
    logger.info(f"  - –û–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(train_data)}")
    logger.info(f"  - –¢–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(test_data)}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    logger.info("–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    logger.info("#1 –ó–∞–≥—Ä—É–∑–∫–∞ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    raw_data = load_raw_data()
    
    if not raw_data:
        logger.error("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return
    
    # 2. –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    logger.info("#2 –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö (—É–¥–∞–ª–µ–Ω–∏–µ –º—É—Å–æ—Ä–∞)")
    cleaned_data = clean_data(raw_data)
    
    if not cleaned_data:
        logger.error("–ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –¥–∞–Ω–Ω—ã—Ö")
        return
    
    # 3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ 80/20 –ø–æ –∫–æ–¥–µ–∫—Å–∞–º
    logger.info("#3 –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö 80/20 –ø–æ –∫–æ–¥–µ–∫—Å–∞–º")
    train_data, test_data = split_by_codes(cleaned_data, test_size=0.2)
    
    # 4. –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¢–û–õ–¨–ö–û –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏
    logger.info("#4 –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏")
    augmented_train = augment_data(train_data)
    
    # 5. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    logger.info("#5 –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö")
    tokenized_train = tokenize_data(augmented_train)
    tokenized_test = tokenize_data(test_data)
    
    # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    logger.info("#6 –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    save_processed_data(tokenized_train, tokenized_test)
    
    logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")

if __name__ == "__main__":
    main()