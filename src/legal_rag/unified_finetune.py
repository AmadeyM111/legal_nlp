#!/usr/bin/env python3
"""
Fine-tuning Saiga Mistral 7B –Ω–∞ Apple Silicon —á–µ—Ä–µ–∑ MLX
–†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ M1/M2/M3 ‚Äî –±—ã—Å—Ç—Ä–æ, —Å—Ç–∞–±–∏–ª—å–Ω–æ, –±–µ–∑ GPU-–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
"""

import json
from pathlib import Path
import argparse
from mlx_lm import load, generate
from mlx_lm.lora import lora, LoraConfig

def main():
    parser = argparse.ArgumentParser(description="Fine-tuning Saiga Mistral 7B –Ω–∞ Mac")
    parser.add_argument("--data", default="data/processed/synthetic_qa_cleaned.json", help="–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É")
    parser.add_argument("--model", default="IlyaGusev/saiga_mistral_7b", help="–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å")
    parser.add_argument("--output", default="models/saiga-legal-mistral-7b-lora", help="–ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å")
    parser.add_argument("--iters", type=int, default=1000, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π")
    parser.add_argument("--batch", type=int, default=4, help="Batch size")
    parser.add_argument("--rank", type=int, default=64, help="LoRA rank")
    args = parser.parse_args()

    # === 1. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ ===
    print("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å (—ç—Ç–æ –∑–∞–π–º—ë—Ç 20‚Äì40 —Å–µ–∫—É–Ω–¥)...")
    model, tokenizer = load(args.model)

    # === 2. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
    data_path = Path(args.data)
    if not data_path.exists():
        print(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {data_path}")
        return

    with open(data_path, "r", encoding="utf-8") as f:
        raw_data = json.load(f)

    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø–æ–¥ —á–∞—Ç
    train_data = []
    for item in raw_data:
        user_msg = item.get("case") or item.get("question", "")
        assistant_msg = item.get("article") or item.get("answer", "")
        if user_msg and assistant_msg:
            train_data.append([
                {"role": "user", "content": user_msg},
                {"role": "assistant", "content": assistant_msg}
            ])

    print(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")

    # === 3. LoRA –∫–æ–Ω—Ñ–∏–≥ ===
    config = LoraConfig(
        rank=args.rank,
        alpha=16,
        dropout=0.05,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    )

    # === 4. Fine-tuning ===
    print(f"–ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ (iters={args.iters}, batch={args.batch})...")
    trained_model = lora(
        model,
        config,
        train_data,
        batch_size=args.batch,
        iters=args.iters,
        learning_rate=2e-4
    )

    # === 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ===
    output_path = Path(args.output)
    output_path.mkdir(parents=True, exist_ok=True)

    print(f"–°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ {output_path}")
    trained_model.save(str(output_path))
    tokenizer.save(str(output_path))

    print("–ì–æ—Ç–æ–≤–æ! –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")
    print(f"–ü—É—Ç—å: {output_path.resolve()}")

    # === 6. –¢–µ—Å—Ç ===


{{ .Response }}<|im_end|
"""
    
    # Save Modelfile
    modelfile_path = PROJECT_ROOT / "Modelfile"
    modelfile_path.write_text(modelfile_content, encoding="utf-8")
    print(f"üìù Modelfile created: {modelfile_path}")

    # Create model in Ollama
    print("üî® Creating base model in Ollama...")
    result = subprocess.run(["ollama", "create", output_model_name, "-f", str(modelfile_path)], 
                           capture_output=True, text=True)

    if result.returncode != 0:
        print("Error creating model:")
        print(result.stderr)
        return False

    print(f"Model {output_model_name} created")

    # Prepare dataset in Ollama format
    train_data = []
    with open(dataset_path, "r", encoding="utf-8") as f:
        raw_data = json.load(f)[:2000]  # Take 2000 examples

    for item in raw_data:
        train_data.append({
            "instruction": "–û–ø—Ä–µ–¥–µ–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º—É—é —Å—Ç–∞—Ç—å—é –∑–∞–∫–æ–Ω–∞ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é –¥–µ–ª–∞",
            "input": item.get("case", item.get("question", "")),
            "output": item.get("article", item.get("output", ""))
        })

    # Save training data
    train_file = PROJECT_ROOT / "train_data.jsonl"
    with open(train_file, "w", encoding="utf-8") as f:
        for item in train_data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

    print(f"üìö Training data saved: {train_file}")

    print(f"‚úÖ Ollama Fine-tuning setup completed! Model: {output_model_name}")
    return True


def main():
    parser = argparse.ArgumentParser(description="Unified Fine-tuning for Legal Models")
    parser.add_argument("--method", choices=["mlx", "ollama"], required=True,
                        help="Fine-tuning method to use")
    parser.add_argument("--dataset", type=str, required=True,
                        help="Path to training dataset")
    parser.add_argument("--model", type=str, default="saiga-mistral-7b",
                        help="Base model name")
    parser.add_argument("--output", type=str,
                        help="Output directory/model name")
    parser.add_argument("--rank", type=int, default=64,
                        help="LoRA rank (MLX only)")
    parser.add_argument("--alpha", type=int, default=16,
                        help="LoRA alpha (MLX only)")
    
    args = parser.parse_args()
    
    if args.method == "mlx":
        output_dir = args.output or f"models/legal-{args.model.replace('/', '-')}-lora"
        success = mlx_finetune(
            dataset_path=args.dataset,
            model_name=args.model,
            output_dir=output_dir,
            rank=args.rank,
            alpha=args.alpha
        )
    elif args.method == "ollama":
        output_model = args.output or "legal-saiga-7b"
        success = ollama_finetune(
            dataset_path=args.dataset,
            output_model_name=output_model
        )
    
    if success:
        print("üéâ Fine-tuning completed successfully!")
    else:
        print("‚ùå Fine-tuning failed!")
        sys.exit(1)


if __name__ == "__main__":
    main()
{{ .Prompt }}<|im_end>
{{ end }}<|im_start|>assistant
SYSTEM }}
{{ .System }}<|im_end>
{{ end }}{{ if .Prompt }}<|im_start|>user
    print("\n–¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å...")
    prompt = "–í —Ç—Ä—É–¥–æ–≤–æ–º –¥–æ–≥–æ–≤–æ—Ä–µ –Ω–µ—Ç —É–¥–∞–ª—ë–Ω–∫–∏. –ú–æ–≥—É –ª–∏ —è —Ä–∞–±–æ—Ç–∞—Ç—å –∏–∑ –¥–æ–º–∞?"
    response = generate(trained_model, tokenizer, prompt=prompt, max_tokens=200, temp=0.3)
    print(f"\n–û—Ç–≤–µ—Ç:\n{response}")

if __name__ == "__main__":
    main()