import json
from transformers import AutoTokenizer
from pathlib import Path
from datasets import load_dataset

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
MODEL_ID = "IlyaGusev/saiga_llama3_8b"
MAX_TOKENS = 7500  # –û—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å –¥–æ 8192 –¥–ª—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –∏ –≤–æ–ø—Ä–æ—Å–∞
OVERLAP = 500      # –°–∫–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ –∫–æ–Ω—Ü–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —á–∞–Ω–∫–∞ –±–µ—Ä–µ–º –≤ –Ω–∞—á–∞–ª–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ
PROJECT_ROOT = Path(__file__).parent.parent.parent
DATA_PATH = PROJECT_ROOT / "data" / "processed" / "all_codes_fixed_qlora.json"
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
train_dataset = load_dataset("json", data_files=DATA_PATH.as_posix(), split="train")

def chunk_text_by_tokens(text, max_tokens, overlap, tokenizer):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –∫—É—Å–∫–∏ –ø–æ —Ç–æ–∫–µ–Ω–∞–º —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º."""
    tokens = tokenizer.encode(text, add_special_tokens=False)
    chunks = []
    
    start = 0
    while start < len(tokens):
        end = start + max_tokens
        chunk_tokens = tokens[start:end]
        chunks.append(tokenizer.decode(chunk_tokens, skip_special_tokens=True))
        
        if end >= len(tokens):
            break
        start += (max_tokens - overlap)
    
    return chunks

# –ó–∞–≥—Ä—É–∑–∫–∞
with open(DATA_PATH, 'r', encoding='utf-8') as f:
    data = json.load(f)

new_dataset = []
chunked_count = 0

print(f"üöÄ –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(data)} –ø—Ä–∏–º–µ—Ä–æ–≤...")

for ex in data:
    user_message = ex["messages"][1]      # –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    assistant_content = ex["messages"][-1]["content"] # –î–ª–∏–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
    
    # –°—á–∏—Ç–∞–µ–º —Ç–æ–∫–µ–Ω—ã —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç–∞
    tokens_count = len(tokenizer.encode(assistant_content))
    
    if tokens_count > MAX_TOKENS:
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏
        text_chunks = chunk_text_by_tokens(assistant_content, MAX_TOKENS, OVERLAP, tokenizer)
        
        for i, chunk in enumerate(text_chunks):
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
            new_ex = {
                "messages": [
                    ex["messages"][0], # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
                    {
                        "role": "user", 
                        "content": f"{user_message['content']} (–ß–∞—Å—Ç—å {i+1}/{len(text_chunks)})"
                    },
                    {"role": "assistant", "content": chunk}
                ]
            }
            new_dataset.append(new_ex)
        chunked_count += 1
    else:
        new_dataset.append(ex)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
with open(DATA_PATH.as_posix(), 'w', encoding='utf-8') as f:
    json.dump(new_dataset, f, ensure_ascii=False, indent=2)

print(f"‚úÖ –ì–æ—Ç–æ–≤–æ!")
print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–ª–∏–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π: {chunked_count}")
print(f"–ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(new_dataset)} (–±—ã–ª–æ {len(data)})")
